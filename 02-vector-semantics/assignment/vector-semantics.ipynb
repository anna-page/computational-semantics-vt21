{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read [the following instructions](https://github.com/sdobnik/computational-semantics/blob/master/README.md) on how to work on group assignments.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for phrases affect both the model and the learned information about similarities.  \n",
    "\n",
    "Note that this lab uses a code from `dist_erk.py`, which contains functions that highly resemble those shown during the lecture. In the end, you can use either of the functions (from the lecture / from the file) to solve the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from that code.\n",
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus which you can download from [here](https://linux.dobnik.net/cloud/index.php/s/isMBj49jt5renYt?path=%2Fresources%2Fa2-distributional-representations) (wikipedia.txt.zip). (This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/)).  \n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below.  \n",
    "<!-- <It may already exist in `/opt/mlt/courses/cl2015/a5`.> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "corpus_dir = os.path.abspath(\"filename/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls. Functions are imported from `dist_erk.py` earlier, and they largely resemble functions shown during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia123.txt\n",
      "reading file wikipedia123.txt\n",
      "create count matrices\n",
      "reading file wikipedia123.txt\n",
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# which words to use as targets and context words?\n",
    "# we need to count the words and keep only the N most frequent ones\n",
    "# which function would you use here with which variable?\n",
    "ktw = do_word_count(corpus_dir, numdims)\n",
    "\n",
    "wi = make_word_index(ktw)\n",
    "\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "\n",
    "make_space(corpus_dir, wi, numdims)\n",
    "\n",
    "# create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# which functions with which arguments would you use here?\n",
    "print('create count matrices')\n",
    "space_1k = make_space(corpus_dir, wi, numdims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(space_1k, 1000, svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. It took 40 minutes on a laptop. We saved all three matrices [here](https://linux.dobnik.net/cloud/index.php/s/isMBj49jt5renYt?path=%2Fresources%2Fa2-distributional-representations) (pretrained.zip). Download them and unpack them to a `pretrained` folder which should be a subfolder of the folder with this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "print('Please wait...')\n",
    "ktw_10k       = np.load('./pretrained/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load('./pretrained/raw_wikipediaktw.npy', allow_pickle=True).all()\n",
    "ppmispace_10k = np.load('./pretrained/ppmi_wikipediaktw.npy', allow_pickle=True).all()\n",
    "svdspace_10k  = np.load('./pretrained/svd50_wikipedia10k.npy', allow_pickle=True).all()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` (a copy is included with this notebook) contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The score range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture (the one from [2]). For more information, please read the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 155\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # it will check if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "        \n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate Pearson's correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(word1, word2, space):\n",
    "    vec1 = space[ word1 ]\n",
    "    vec2 = space[word2]\n",
    "\n",
    "    veclen1 = veclen(vec1)\n",
    "    veclen2 = veclen(vec2)\n",
    "\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty. make the cosine zero.\n",
    "        return 0.0\n",
    "\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        # dotproduct = numpy.dot(vec1, vec2)\n",
    "        dotproduct = numpy.sum(vec1 * vec2)\n",
    "\n",
    "        return dotproduct / (veclen1 * veclen2)\n",
    "\n",
    "raw_similarities  = [cosine(w1, w2, space_10k) for w1, w2 in word_pairs]\n",
    "ppmi_similarities = [cosine(w1, w2, ppmispace_10k) for w1, w2 in word_pairs]\n",
    "svd_similarities  = [cosine(w1, w2, svdspace_10k) for w1, w2 in word_pairs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity vs Raw Similarity:\n",
      "rho     = 0.1522\n",
      "p-value = 0.0000\n",
      "Semantic Similarity vs PPMI Similarity:\n",
      "rho     = 0.4547\n",
      "p-value = 0.0000\n",
      "Semantic Similarity vs SVD Similarity:\n",
      "rho     = 0.4232\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, raw_similarities)\n",
    "print(\"\"\"Semantic Similarity vs Raw Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, ppmi_similarities)\n",
    "print(\"\"\"Semantic Similarity vs PPMI Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, svd_similarities)\n",
    "print(\"\"\"Semantic Similarity vs SVD Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PPMI model correlated the similarity scores best with the real semantic similarity scores from the internet. However, the difference between the PPMI score and the SVD score appears to be marginal. This outcome is expected because the PPMI score takes into consideration how distinctive a word is in a given context, i.e. it minimises the importance of words that occur frrequently across all contexts. Additionally, SVD reduces noise/randomness in the data to help find a 'true' signal or information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs Raw Similarity:\n",
      "rho     = 0.1212\n",
      "p-value = 0.0007\n",
      "Visual Similarity vs. PPMI Similarity:\n",
      "rho     = 0.3838\n",
      "p-value = 0.0000\n",
      "Visual Similarity vs. SVD Similarity:\n",
      "rho     = 0.3097\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "rho, pval = stats.spearmanr(visual_similarity, raw_similarities)\n",
    "print(\"\"\"Visual Similarity vs Raw Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(visual_similarity, ppmi_similarities)\n",
    "print(\"\"\"Visual Similarity vs. PPMI Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(visual_similarity, svd_similarities)\n",
    "print(\"\"\"Visual Similarity vs. SVD Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the three matrices compared against the visual similarity ranks in the same order as the semantic similarity above however they perform lower across the board. The PPMI and SVD matrices performance drops off more sharply than the raw similarities, though they still outperform the raw similarity by a considerable margin. It is unsuprising that the performance overall is lower - the matrices were never taking visual similarity into account. It is a bit surprising that he raw similarity's performance does not drop off more than it did. This may indicate that the performance of the raw similarity was never particularly good to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions. For example, we can subtract the normalised vectors for `king` minus `queen` and add the resulting vector to `man` and we hope to get the vector for `woman`. Why? **[3 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is incorrect, but we can still apply this principle. We know in advance that the primary difference between a king and a queen (in terms of real entities) is gender. We can attempt to capture this semantic information in the vector space literally by taking the difference between the vectors of the two entities.\n",
    "\n",
    "Thus we hope that (king-queen) gives us the mapping from king to queen in the vector space, where our key assumption is that the mapping IS the gender difference.\n",
    "\n",
    "If we have a vector as the semantic representation of gender, which can map between similar types of entities/nouns which only differ in gender, then ((queen-king)+man) should approximate (woman), if the respective differences between king and queen, and men and women are their gender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some helpful code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 0.8733111261346901),\n",
       " ('above', 0.8259671977311955),\n",
       " ('around', 0.8030776291120685),\n",
       " ('sun', 0.7692439111243973),\n",
       " ('just', 0.7678481974778111),\n",
       " ('wide', 0.767257431992253),\n",
       " ('each', 0.7665960260861158),\n",
       " ('circle', 0.7647746702909336),\n",
       " ('length', 0.7601066921319761),\n",
       " ('almost', 0.7542351860536628)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "find_similar_to(light - (heavy - long), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector of short is not among the 10 most similar vectors, perhaps the relation between light and heavy is not similar enough to the relation between short and long for this to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: Google for `word analogies examples`. You can also construct analogies that are less lexical but more grammatical, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) from [3]. Does the resulting vector similarity confirm your expectations? But remember you can only do this if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stockholm = normalize(svdspace_10k['stockholm'])\n",
    "sweden = normalize(svdspace_10k['sweden'])\n",
    "copenhagen = normalize(svdspace_10k['copenhagen'])\n",
    "denmark = normalize(svdspace_10k['denmark'])\n",
    "\n",
    "\n",
    "south = normalize(svdspace_10k['south'])\n",
    "north = normalize(svdspace_10k['north'])\n",
    "east = normalize(svdspace_10k['east'])\n",
    "west = normalize(svdspace_10k['west'])\n",
    "\n",
    "\n",
    "\n",
    "week = normalize(svdspace_10k['week'])\n",
    "work = normalize(svdspace_10k['work'])\n",
    "fun = normalize(svdspace_10k['fun'])\n",
    "weedend = normalize(svdspace_10k['weekend'])\n",
    "\n",
    "\n",
    "up = normalize(svdspace_10k['up'])\n",
    "high = normalize(svdspace_10k['high'])\n",
    "down = normalize(svdspace_10k['down'])\n",
    "low = normalize(svdspace_10k['low'])\n",
    "\n",
    "\n",
    "man = normalize(svdspace_10k['man'])\n",
    "woman = normalize(svdspace_10k['woman'])\n",
    "suit = normalize(svdspace_10k['suit'])\n",
    "dress = normalize(svdspace_10k['dress'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sweden', 0.9182356045804703),\n",
       " ('denmark', 0.8580303489267215),\n",
       " ('france', 0.8440013978711486),\n",
       " ('spain', 0.8301392747312987),\n",
       " ('portugal', 0.8213020089711681),\n",
       " ('austria', 0.8207164058301429),\n",
       " ('ireland', 0.8185169771255273),\n",
       " ('russia', 0.8167576409081696),\n",
       " ('belgium', 0.813768120248223),\n",
       " ('scotland', 0.8091028834377446)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_to(sweden - (stockholm - copenhagen), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('north', 0.9771493003083315),\n",
       " ('west', 0.9261105860453677),\n",
       " ('94', 0.8684855057550237),\n",
       " ('59', 0.8567001838313205),\n",
       " ('81', 0.8508666429149574),\n",
       " ('47', 0.8498260606055098),\n",
       " ('51', 0.8485466875663064),\n",
       " ('83', 0.8476736758957539),\n",
       " ('57', 0.8472095348639865),\n",
       " ('97', 0.846783792744607)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_to(north - (south - east), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('midnight', 0.7593186401972436),\n",
       " ('week', 0.7410206061707413),\n",
       " ('tonight', 0.73755040038101),\n",
       " ('weekend', 0.7337554900094316),\n",
       " ('lucky', 0.7174816015710194),\n",
       " ('lonely', 0.7124418012899021),\n",
       " ('afternoon', 0.7107110404742536),\n",
       " ('dirty', 0.7092583029180727),\n",
       " ('fun', 0.7088023835686386),\n",
       " ('tomorrow', 0.7016167908607219)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_to(((week - work) + fun), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('down', 0.9228344781988292),\n",
       " ('back', 0.8866341446782393),\n",
       " ('off', 0.8755815493034719),\n",
       " ('away', 0.8669123137105386),\n",
       " ('up', 0.8385302599781487),\n",
       " ('just', 0.8143869669353988),\n",
       " ('behind', 0.8114415466351721),\n",
       " ('front', 0.8048615445972288),\n",
       " ('left', 0.7870710896231837),\n",
       " ('home', 0.7829769169141294)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_to(((down - low) + high), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suit', 0.9416501644222036),\n",
       " ('suits', 0.8511995979510889),\n",
       " ('trademark', 0.7799054556696589),\n",
       " ('costume', 0.7650391997980102),\n",
       " ('holder', 0.7605134204463706),\n",
       " ('owners', 0.7565212650600517),\n",
       " ('stamp', 0.749830644173649),\n",
       " ('contract', 0.7466969582149255),\n",
       " ('ballot', 0.7447614655933226),\n",
       " ('dress', 0.7425159418166167)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_to(woman-man + suit, svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each case our expected word did occur but not as the most similar word to the resulting vector, however it did typically occur in the top half with a fairly high similarity. The one that is different is (woman-man+suit) where dress occurs at the bottom of the list however if you look at the likelihood score for it its atually not a particularly low score at 0.742.. for example the vector manipulation involving work and week + fun, even the most similar vector is only ranked .02 higher than the one for dress. Despite being at the bottom of the list its not like it has decided that it is unlikely, its just that 9 other vectors are more likely in this case. This and the results for the other ones raises the question of how to select the vector that we actually want since its never the top one. \n",
    "\n",
    "\n",
    "The two elements that are similar adds a smaller contribution to the resulting vector than the dissimilar element. For example, in (woman - man + suit) man and woman are more similar vectors and so the resulting vector will be more similar to suit than man or woman.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to look at how different semantic composition models, introduced in [2] correlate with human judgements. The file with the dataset is `mitchell_lapata_acl08.txt` included with this notebook. Your task is to do the following:  \n",
    "\n",
    "(i) process the dataset, extract pairs of `reference - landmark high` and `reference - landmark low`; you can use the code from the lecture as something to start with. Note that there are 2 landmarks for each reference: one landmark exhibits high similarity with the reference, while another one has low similarity with the reference. A single human participant could have evaluated both of these pairs. For more details, we refer you to the paper.  \n",
    "\n",
    "(ii) build models of semantic phrase composition: in the lecture we introduced simple additive, simple multiplicative and combined models (details are in [2]). Your task is to take a single pair (a reference or a high similarity landmark or a low similarity landmark) and compute the composition of its vectors using each of these functions. Thus, you will have three compositional models that take a `noun - verb` pair and output a single vector, representing the meaning of this pair. As your semantic space, you can use pretrained spaces (standard space, ppmi or svd) introduced above. It is up to you which space you use, but for someone who runs your code, it should be pretty straightforward to switch between them.\n",
    "\n",
    "(iii) calculate Spearman correlation between each model's predictions and human judgements; you should have something similar to the scores that are shown in the paper [2]:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./res.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper states that they calculated correlations between each individual participant's judgeements and each model's predictions.  \n",
    "\n",
    "Let's say we have 3 models: simple additive (A), simple multiplicative (M), combined (C).  \n",
    "From our task dataset, we also know that we have 20 participants.  \n",
    "Now, for each participant in 20 participants we get all `verb - noun` pairs that these participated evaluated.  \n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_judgemenets_example = [\n",
    " 'participant50 chatter child gabble 6 high',\n",
    " 'participant50 chatter tooth click 2 high',\n",
    " 'participant50 reel head whirl 5 high',\n",
    " 'participant50 reel mind stagger 4 low',\n",
    " 'participant50 reel industry stagger 5 high',\n",
    " 'participant50 reel man whirl 3 low',\n",
    " 'participant50 glow fire beam 7 low',\n",
    " 'participant50 glow face burn 3 low',\n",
    " 'participant50 glow cigar burn 5 high',\n",
    " 'participant50 glow skin beam 7 high'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant50 chatter child gabble 6 high',\n",
       " 'participant50 chatter tooth click 2 high',\n",
       " 'participant50 reel head whirl 5 high',\n",
       " 'participant50 reel mind stagger 4 low',\n",
       " 'participant50 reel industry stagger 5 high',\n",
       " 'participant50 reel man whirl 3 low',\n",
       " 'participant50 glow fire beam 7 low',\n",
       " 'participant50 glow face burn 3 low',\n",
       " 'participant50 glow cigar burn 5 high',\n",
       " 'participant50 glow skin beam 7 high']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_judgemenets_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first pair that participant50 evaluated: reference `child chatter` and high-level similarity landmark (as the last word in the row indicates) `child gabble`. The human gave the similarity score of 6 (very similar). Thus, human similarity judgment = [6].  \n",
    "\n",
    "Our A model's output:  \n",
    "cosine(p1, p2) = 0.88, where p1 is the result of addition of word vectors in the reference phrase `child gabble`, and p2 is the result of addition of word vectors in the high-level similarity phrase `child chatter`.  \n",
    "\n",
    "Therefore, we have human rating vector [6] and model A output [0.88]. Next is to compute correlation between these two vectors.\n",
    "\n",
    "To get an overall score, simply average your correlation scores over all participants, since you are calculating correlation scores per participant.\n",
    "\n",
    "Of course, your human rating vectors will be longer (e.g., [6, 7, 3, 4, 5]) where each element is a participant's judgement of a specific pair. Each of your models (A, B, C) will produce a single vector of cosine similarity between these same pairs (e.g., [0.89, 0.98, 0.23, 0.65, 0.55]). The goal is to compare each model's cosine similarity vectors with human rating vectors and identify the model which outputs the best result in terms of being the closest to the way human rate similarity between the phrases.\n",
    "\n",
    "The minimum to do in this task: compute correlations for 3 models mentioned above and human rating for AT LEAST one participant. Elaborate on how different the resulting correlation scores are depending on the model's composition function (additive, multiplicative, combined). For examples on how to interpret the results, look at Section 5 Results of the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia123.txt\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "ktw = do_word_count(corpus_dir, 20000)\n",
    "\n",
    "def preprocess(s):\n",
    "    # split up into words, lowercase, remove punctuation at beginning and end of word\n",
    "    return [x.lower() for x in s if x not in stopwords.words('english') and x not in string.punctuation]\n",
    "\n",
    "ktw = preprocess(ktw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia123.txt\n"
     ]
    }
   ],
   "source": [
    "wi2 = make_word_index(ktw)\n",
    "space_20k = make_space(corpus_dir, wi2, 19865)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "participant20 stray thought roam 7 low\n",
      "participant20 stray discussion digress 6 high\n",
      "participant20 stray eye roam 7 high\n",
      "participant20 stray child digress 1 low\n",
      "participant20 throb body pulse 5 high\n",
      "participant20 throb head shudder 2 low\n",
      "participant20 throb voice shudder 3 low\n",
      "participant20 throb vein pulse 6 high\n",
      "participant20 chatter machine click 4 high\n"
     ]
    }
   ],
   "source": [
    "# load the task dataset\n",
    "with open('./mitchell_lapata_acl08.txt', 'r') as f:\n",
    "    phrase_dataset = f.read().splitlines()\n",
    "\n",
    "for line in phrase_dataset[:10]:\n",
    "    print(line)\n",
    "    \n",
    "# get all unique words\n",
    "words = []\n",
    "for line in phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in words:\n",
    "        words.append(verb)\n",
    "    if noun not in words:\n",
    "        words.append(noun)\n",
    "    if landmark not in words:\n",
    "        words.append(landmark)\n",
    "\n",
    "\n",
    "\n",
    "#this is our corpus, if words not in corpus remove\n",
    "our_dataset = ktw\n",
    "\n",
    "to_remove = []\n",
    "for w in words:\n",
    "    if w not in our_dataset:\n",
    "        to_remove.append(w)\n",
    "        \n",
    "\n",
    "cleaned_phrase_dataset = []\n",
    "for line in phrase_dataset:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb in to_remove or noun in to_remove or landmark in to_remove:\n",
    "        continue\n",
    "    cleaned_phrase_dataset.append(line)\n",
    "\n",
    "target_words = []\n",
    "for line in cleaned_phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in target_words:\n",
    "        target_words.append(verb)\n",
    "    if noun not in target_words:\n",
    "        target_words.append(noun)\n",
    "    if landmark not in target_words:\n",
    "        target_words.append(landmark)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['participant verb noun landmark input hilo', 'participant20 bow butler submit 3 low', 'participant20 bow company submit 5 high', 'participant20 boom noise prosper 3 low', 'participant20 boom export prosper 7 high', 'participant20 boom sale thunder 3 low', 'participant20 boom gun thunder 6 high', 'participant20 glow fire burn 5 high', 'participant20 glow face beam 7 high', 'participant20 glow skin burn 1 low', 'participant21 bow butler submit 2 low', 'participant21 bow company submit 6 high', 'participant21 boom noise prosper 1 low', 'participant21 boom export prosper 6 high', 'participant21 boom gun thunder 6 high', 'participant21 boom sale thunder 5 low', 'participant21 glow fire burn 5 high', 'participant21 glow face beam 7 high', 'participant21 glow skin burn 2 low', 'participant22 bow butler submit 2 low', 'participant22 bow company submit 6 high', 'participant22 boom noise prosper 1 low', 'participant22 boom export prosper 6 high', 'participant22 boom gun thunder 7 high', 'participant22 boom sale thunder 2 low', 'participant22 glow fire burn 4 high', 'participant22 glow skin burn 3 low', 'participant22 glow face beam 6 high', 'participant23 bow butler submit 3 low', 'participant23 bow company submit 4 high', 'participant23 boom noise prosper 3 low', 'participant23 boom export prosper 6 high', 'participant23 boom sale thunder 3 low', 'participant23 boom gun thunder 5 high', 'participant23 glow fire burn 4 high', 'participant23 glow face beam 2 high', 'participant23 glow skin burn 2 low', 'participant24 bow butler submit 4 low', 'participant24 bow company submit 6 high', 'participant24 boom noise prosper 4 low', 'participant24 boom export prosper 6 high', 'participant24 boom gun thunder 6 high', 'participant24 boom sale thunder 1 low', 'participant24 glow fire burn 4 high', 'participant24 glow face beam 6 high', 'participant24 glow skin burn 1 low', 'participant25 bow butler submit 5 low', 'participant25 bow company submit 5 high', 'participant25 boom noise prosper 2 low', 'participant25 boom export prosper 6 high', 'participant25 boom gun thunder 6 high', 'participant25 boom sale thunder 2 low', 'participant25 glow fire burn 2 high', 'participant25 glow skin burn 2 low', 'participant25 glow face beam 4 high', 'participant26 bow butler submit 4 low', 'participant26 bow company submit 5 high', 'participant26 boom noise prosper 2 low', 'participant26 boom export prosper 3 high', 'participant26 boom gun thunder 5 high', 'participant26 boom sale thunder 2 low', 'participant26 glow fire burn 4 high', 'participant26 glow skin burn 1 low', 'participant26 glow face beam 5 high', 'participant27 bow butler submit 2 low', 'participant27 bow company submit 4 high', 'participant27 boom noise prosper 2 low', 'participant27 boom export prosper 7 high', 'participant27 boom sale thunder 2 low', 'participant27 boom gun thunder 6 high', 'participant27 glow fire burn 5 high', 'participant27 glow face beam 7 high', 'participant27 glow skin burn 3 low', 'participant28 bow butler submit 2 low', 'participant28 bow company submit 6 high', 'participant28 boom noise prosper 1 low', 'participant28 boom export prosper 7 high', 'participant28 boom sale thunder 4 low', 'participant28 boom gun thunder 6 high', 'participant28 glow fire burn 2 high', 'participant28 glow skin burn 1 low', 'participant28 glow face beam 3 high', 'participant29 bow butler submit 2 low', 'participant29 bow company submit 2 high', 'participant29 boom noise prosper 2 low', 'participant29 boom export prosper 6 high', 'participant29 boom gun thunder 6 high', 'participant29 boom sale thunder 5 low', 'participant29 glow fire burn 3 high', 'participant29 glow face beam 4 high', 'participant29 glow skin burn 1 low', 'participant36 bow head submit 3 low', 'participant36 bow government submit 6 high', 'participant36 boom noise thunder 6 high', 'participant36 boom export thunder 2 low', 'participant36 boom gun prosper 1 low', 'participant36 boom sale prosper 6 high', 'participant36 glow fire beam 2 low', 'participant36 glow skin beam 3 high', 'participant36 glow face burn 5 low', 'participant46 bow head submit 4 low', 'participant46 bow government submit 6 high', 'participant46 boom noise thunder 6 high', 'participant46 boom export thunder 4 low', 'participant46 boom sale prosper 7 high', 'participant46 boom gun prosper 1 low', 'participant46 glow skin beam 6 high', 'participant46 glow fire beam 3 low', 'participant46 glow face burn 7 low', 'participant47 bow head submit 3 low', 'participant47 bow government submit 3 high', 'participant47 boom noise thunder 6 high', 'participant47 boom export thunder 3 low', 'participant47 boom gun prosper 4 low', 'participant47 boom sale prosper 6 high', 'participant47 glow fire beam 2 low', 'participant47 glow face burn 3 low', 'participant47 glow skin beam 5 high', 'participant44 bow head submit 3 low', 'participant44 bow government submit 7 high', 'participant44 boom noise thunder 7 high', 'participant44 boom export thunder 5 low', 'participant44 boom gun prosper 5 low', 'participant44 boom sale prosper 7 high', 'participant44 glow fire beam 3 low', 'participant44 glow face burn 3 low', 'participant44 glow skin beam 6 high', 'participant45 bow head submit 5 low', 'participant45 bow government submit 7 high', 'participant45 boom noise thunder 7 high', 'participant45 boom export thunder 2 low', 'participant45 boom gun prosper 1 low', 'participant45 boom sale prosper 6 high', 'participant45 glow skin beam 6 high', 'participant45 glow fire beam 5 low', 'participant45 glow face burn 4 low', 'participant42 bow head submit 2 low', 'participant42 bow government submit 5 high', 'participant42 boom noise thunder 7 high', 'participant42 boom export thunder 3 low', 'participant42 boom sale prosper 7 high', 'participant42 boom gun prosper 3 low', 'participant42 glow fire beam 4 low', 'participant42 glow face burn 4 low', 'participant42 glow skin beam 7 high', 'participant43 bow head submit 5 low', 'participant43 bow government submit 3 high', 'participant43 boom noise thunder 6 high', 'participant43 boom export thunder 5 low', 'participant43 boom gun prosper 4 low', 'participant43 boom sale prosper 7 high', 'participant43 glow fire beam 1 low', 'participant43 glow face burn 5 low', 'participant43 glow skin beam 7 high', 'participant40 bow head submit 5 low', 'participant40 bow government submit 2 high', 'participant40 boom noise thunder 6 high', 'participant40 boom export thunder 3 low', 'participant40 boom gun prosper 3 low', 'participant40 boom sale prosper 6 high', 'participant40 glow fire beam 5 low', 'participant40 glow face burn 2 low', 'participant40 glow skin beam 5 high', 'participant41 bow head submit 1 low', 'participant41 bow government submit 5 high', 'participant41 boom noise thunder 6 high', 'participant41 boom export thunder 2 low', 'participant41 boom gun prosper 1 low', 'participant41 boom sale prosper 7 high', 'participant41 glow fire beam 5 low', 'participant41 glow face burn 3 low', 'participant41 glow skin beam 6 high', 'participant48 bow head submit 2 low', 'participant48 bow government submit 5 high', 'participant48 boom noise thunder 5 high', 'participant48 boom export thunder 1 low', 'participant48 boom sale prosper 7 high', 'participant48 boom gun prosper 1 low', 'participant48 glow fire beam 2 low', 'participant48 glow face burn 1 low', 'participant48 glow skin beam 2 high', 'participant49 bow head submit 3 low', 'participant49 bow government submit 6 high', 'participant49 boom noise thunder 5 high', 'participant49 boom export thunder 4 low', 'participant49 boom gun prosper 1 low', 'participant49 boom sale prosper 7 high', 'participant49 glow fire beam 3 low', 'participant49 glow face burn 1 low', 'participant49 glow skin beam 7 high', 'participant37 bow head submit 6 low', 'participant37 bow government submit 7 high', 'participant37 boom noise thunder 6 high', 'participant37 boom export thunder 2 low', 'participant37 boom gun prosper 3 low', 'participant37 boom sale prosper 7 high', 'participant37 glow fire beam 4 low', 'participant37 glow face burn 7 low', 'participant37 glow skin beam 6 high', 'participant19 bow butler submit 3 low', 'participant19 bow company submit 2 high', 'participant19 boom noise prosper 3 low', 'participant19 boom export prosper 7 high', 'participant19 boom sale thunder 3 low', 'participant19 boom gun thunder 6 high', 'participant19 glow fire burn 4 high', 'participant19 glow face beam 6 high', 'participant19 glow skin burn 2 low', 'participant18 bow butler submit 5 low', 'participant18 bow company submit 5 high', 'participant18 boom noise prosper 4 low', 'participant18 boom export prosper 6 high', 'participant18 boom gun thunder 6 high', 'participant18 boom sale thunder 2 low', 'participant18 glow fire burn 4 high', 'participant18 glow face beam 5 high', 'participant18 glow skin burn 2 low', 'participant35 bow head submit 2 low', 'participant35 bow government submit 6 high', 'participant35 boom noise thunder 7 high', 'participant35 boom export thunder 1 low', 'participant35 boom sale prosper 5 high', 'participant35 boom gun prosper 1 low', 'participant35 glow fire beam 2 low', 'participant35 glow skin beam 4 high', 'participant35 glow face burn 2 low', 'participant34 bow butler submit 4 low', 'participant34 bow company submit 6 high', 'participant34 boom noise prosper 4 low', 'participant34 boom export prosper 6 high', 'participant34 boom gun thunder 6 high', 'participant34 boom sale thunder 4 low', 'participant34 glow fire burn 6 high', 'participant34 glow face beam 5 high', 'participant34 glow skin burn 4 low', 'participant33 bow butler submit 2 low', 'participant33 bow company submit 6 high', 'participant33 boom noise prosper 2 low', 'participant33 boom export prosper 6 high', 'participant33 boom gun thunder 7 high', 'participant33 boom sale thunder 1 low', 'participant33 glow fire burn 6 high', 'participant33 glow face beam 7 high', 'participant33 glow skin burn 3 low', 'participant32 bow butler submit 2 low', 'participant32 bow company submit 2 high', 'participant32 boom noise prosper 3 low', 'participant32 boom export prosper 6 high', 'participant32 boom gun thunder 4 high', 'participant32 boom sale thunder 3 low', 'participant32 glow fire burn 5 high', 'participant32 glow face beam 6 high', 'participant32 glow skin burn 4 low', 'participant31 bow butler submit 4 low', 'participant31 bow company submit 4 high', 'participant31 boom noise prosper 1 low', 'participant31 boom export prosper 2 high', 'participant31 boom gun thunder 6 high', 'participant31 boom sale thunder 2 low', 'participant31 glow fire burn 5 high', 'participant31 glow face beam 6 high', 'participant31 glow skin burn 2 low', 'participant30 bow butler submit 6 low', 'participant30 bow company submit 4 high', 'participant30 boom noise prosper 1 low', 'participant30 boom export prosper 5 high', 'participant30 boom gun thunder 7 high', 'participant30 boom sale thunder 3 low', 'participant30 glow fire burn 5 high', 'participant30 glow skin burn 1 low', 'participant30 glow face beam 7 high', 'participant11 bow butler submit 3 low', 'participant11 bow company submit 6 high', 'participant11 boom noise prosper 1 low', 'participant11 boom export prosper 6 high', 'participant11 boom sale thunder 4 low', 'participant11 boom gun thunder 6 high', 'participant11 glow fire burn 4 high', 'participant11 glow skin burn 3 low', 'participant11 glow face beam 6 high', 'participant10 bow butler submit 7 low', 'participant10 bow company submit 7 high', 'participant10 boom noise prosper 2 low', 'participant10 boom export prosper 6 high', 'participant10 boom sale thunder 6 low', 'participant10 boom gun thunder 7 high', 'participant10 glow fire burn 7 high', 'participant10 glow face beam 6 high', 'participant10 glow skin burn 7 low', 'participant13 bow butler submit 1 low', 'participant13 bow company submit 6 high', 'participant13 boom noise prosper 5 low', 'participant13 boom export prosper 6 high', 'participant13 boom gun thunder 5 high', 'participant13 boom sale thunder 5 low', 'participant13 glow fire burn 5 high', 'participant13 glow face beam 2 high', 'participant13 glow skin burn 3 low', 'participant12 bow butler submit 2 low', 'participant12 bow company submit 6 high', 'participant12 boom noise prosper 2 low', 'participant12 boom export prosper 2 high', 'participant12 boom sale thunder 2 low', 'participant12 boom gun thunder 7 high', 'participant12 glow fire burn 4 high', 'participant12 glow skin burn 3 low', 'participant12 glow face beam 5 high', 'participant15 bow butler submit 2 low', 'participant15 bow company submit 2 high', 'participant15 boom noise prosper 2 low', 'participant15 boom export prosper 4 high', 'participant15 boom sale thunder 2 low', 'participant15 boom gun thunder 6 high', 'participant15 glow fire burn 6 high', 'participant15 glow skin burn 1 low', 'participant15 glow face beam 5 high', 'participant14 bow butler submit 5 low', 'participant14 bow company submit 3 high', 'participant14 boom noise prosper 2 low', 'participant14 boom export prosper 4 high', 'participant14 boom sale thunder 1 low', 'participant14 boom gun thunder 3 high', 'participant14 glow fire burn 4 high', 'participant14 glow face beam 6 high', 'participant14 glow skin burn 2 low', 'participant17 bow butler submit 2 low', 'participant17 bow company submit 3 high', 'participant17 boom noise prosper 1 low', 'participant17 boom export prosper 7 high', 'participant17 boom sale thunder 3 low', 'participant17 boom gun thunder 6 high', 'participant17 glow fire burn 3 high', 'participant17 glow skin burn 3 low', 'participant17 glow face beam 7 high', 'participant16 bow butler submit 2 low', 'participant16 bow company submit 2 high', 'participant16 boom noise prosper 3 low', 'participant16 boom export prosper 1 high', 'participant16 boom gun thunder 4 high', 'participant16 boom sale thunder 1 low', 'participant16 glow fire burn 3 high', 'participant16 glow skin burn 2 low', 'participant16 glow face beam 6 high', 'participant55 bow head submit 4 low', 'participant55 bow government submit 6 high', 'participant55 boom noise thunder 7 high', 'participant55 boom export thunder 2 low', 'participant55 boom gun prosper 1 low', 'participant55 boom sale prosper 7 high', 'participant55 glow fire beam 5 low', 'participant55 glow skin beam 3 high', 'participant55 glow face burn 5 low', 'participant54 bow head submit 4 low', 'participant54 bow government submit 7 high', 'participant54 boom noise thunder 6 high', 'participant54 boom export thunder 1 low', 'participant54 boom sale prosper 7 high', 'participant54 boom gun prosper 1 low', 'participant54 glow skin beam 4 high', 'participant54 glow fire beam 4 low', 'participant54 glow face burn 4 low', 'participant57 bow head submit 4 low', 'participant57 bow government submit 6 high', 'participant57 boom noise thunder 7 high', 'participant57 boom export thunder 2 low', 'participant57 boom gun prosper 1 low', 'participant57 boom sale prosper 7 high', 'participant57 glow fire beam 2 low', 'participant57 glow skin beam 7 high', 'participant57 glow face burn 1 low', 'participant56 bow head submit 1 low', 'participant56 bow government submit 3 high', 'participant56 boom noise thunder 6 high', 'participant56 boom export thunder 1 low', 'participant56 boom sale prosper 7 high', 'participant56 boom gun prosper 1 low', 'participant56 glow fire beam 2 low', 'participant56 glow skin beam 1 high', 'participant56 glow face burn 4 low', 'participant51 bow head submit 1 low', 'participant51 bow government submit 7 high', 'participant51 boom noise thunder 7 high', 'participant51 boom export thunder 1 low', 'participant51 boom sale prosper 7 high', 'participant51 boom gun prosper 1 low', 'participant51 glow fire beam 1 low', 'participant51 glow face burn 1 low', 'participant51 glow skin beam 6 high', 'participant39 bow head submit 1 low', 'participant39 bow government submit 4 high', 'participant39 boom noise thunder 5 high', 'participant39 boom export thunder 4 low', 'participant39 boom sale prosper 4 high', 'participant39 boom gun prosper 1 low', 'participant39 glow fire beam 4 low', 'participant39 glow skin beam 5 high', 'participant39 glow face burn 6 low', 'participant53 bow head submit 3 low', 'participant53 bow government submit 6 high', 'participant53 boom noise thunder 7 high', 'participant53 boom export thunder 6 low', 'participant53 boom gun prosper 4 low', 'participant53 boom sale prosper 7 high', 'participant53 glow fire beam 7 low', 'participant53 glow face burn 7 low', 'participant53 glow skin beam 7 high', 'participant52 bow head submit 3 low', 'participant52 bow government submit 3 high', 'participant52 boom noise thunder 6 high', 'participant52 boom export thunder 1 low', 'participant52 boom gun prosper 2 low', 'participant52 boom sale prosper 6 high', 'participant52 glow fire beam 1 low', 'participant52 glow skin beam 2 high', 'participant52 glow face burn 4 low', 'participant60 bow head submit 4 low', 'participant60 bow government submit 2 high', 'participant60 boom noise thunder 7 high', 'participant60 boom export thunder 3 low', 'participant60 boom gun prosper 4 low', 'participant60 boom sale prosper 7 high', 'participant60 glow skin beam 7 high', 'participant60 glow fire beam 2 low', 'participant60 glow face burn 7 low', 'participant38 bow head submit 5 low', 'participant38 bow government submit 7 high', 'participant38 boom noise thunder 5 high', 'participant38 boom export thunder 2 low', 'participant38 boom sale prosper 7 high', 'participant38 boom gun prosper 2 low', 'participant38 glow fire beam 5 low', 'participant38 glow skin beam 3 high', 'participant38 glow face burn 3 low', 'participant59 bow head submit 2 low', 'participant59 bow government submit 7 high', 'participant59 boom noise thunder 6 high', 'participant59 boom export thunder 3 low', 'participant59 boom sale prosper 5 high', 'participant59 boom gun prosper 1 low', 'participant59 glow fire beam 2 low', 'participant59 glow skin beam 2 high', 'participant59 glow face burn 4 low', 'participant58 bow head submit 7 low', 'participant58 bow government submit 7 high', 'participant58 boom noise thunder 7 high', 'participant58 boom export thunder 5 low', 'participant58 boom sale prosper 7 high', 'participant58 boom gun prosper 1 low', 'participant58 glow fire beam 6 low', 'participant58 glow skin beam 6 high', 'participant58 glow face burn 7 low', 'participant1 bow butler submit 1 low', 'participant1 bow company submit 4 high', 'participant1 boom noise prosper 1 low', 'participant1 boom export prosper 6 high', 'participant1 boom sale thunder 1 low', 'participant1 boom gun thunder 3 high', 'participant1 glow fire burn 6 high', 'participant1 glow skin burn 5 low', 'participant1 glow face beam 4 high', 'participant3 bow butler submit 2 low', 'participant3 bow company submit 2 high', 'participant3 boom noise prosper 1 low', 'participant3 boom export prosper 5 high', 'participant3 boom sale thunder 2 low', 'participant3 boom gun thunder 3 high', 'participant3 glow fire burn 5 high', 'participant3 glow skin burn 2 low', 'participant3 glow face beam 6 high', 'participant2 bow butler submit 3 low', 'participant2 bow company submit 5 high', 'participant2 boom noise prosper 3 low', 'participant2 boom export prosper 6 high', 'participant2 boom gun thunder 6 high', 'participant2 boom sale thunder 2 low', 'participant2 glow fire burn 6 high', 'participant2 glow face beam 6 high', 'participant2 glow skin burn 2 low', 'participant5 bow butler submit 3 low', 'participant5 bow company submit 6 high', 'participant5 boom noise prosper 1 low', 'participant5 boom export prosper 6 high', 'participant5 boom sale thunder 2 low', 'participant5 boom gun thunder 6 high', 'participant5 glow fire burn 4 high', 'participant5 glow face beam 7 high', 'participant5 glow skin burn 3 low', 'participant4 bow butler submit 1 low', 'participant4 bow company submit 2 high', 'participant4 boom noise prosper 1 low', 'participant4 boom export prosper 5 high', 'participant4 boom sale thunder 1 low', 'participant4 boom gun thunder 7 high', 'participant4 glow skin burn 2 low', 'participant4 glow fire burn 5 high', 'participant4 glow face beam 4 high', 'participant7 bow butler submit 1 low', 'participant7 bow company submit 5 high', 'participant7 boom noise prosper 1 low', 'participant7 boom export prosper 6 high', 'participant7 boom gun thunder 5 high', 'participant7 boom sale thunder 6 low', 'participant7 glow fire burn 1 high', 'participant7 glow face beam 5 high', 'participant7 glow skin burn 1 low', 'participant6 bow butler submit 1 low', 'participant6 bow company submit 2 high', 'participant6 boom noise prosper 3 low', 'participant6 boom export prosper 7 high', 'participant6 boom gun thunder 6 high', 'participant6 boom sale thunder 3 low', 'participant6 glow fire burn 3 high', 'participant6 glow face beam 6 high', 'participant6 glow skin burn 4 low', 'participant9 bow butler submit 4 low', 'participant9 bow company submit 5 high', 'participant9 boom noise prosper 2 low', 'participant9 boom export prosper 7 high', 'participant9 boom sale thunder 4 low', 'participant9 boom gun thunder 7 high', 'participant9 glow fire burn 6 high', 'participant9 glow face beam 5 high', 'participant9 glow skin burn 2 low', 'participant8 bow butler submit 5 low', 'participant8 bow company submit 3 high', 'participant8 boom noise prosper 2 low', 'participant8 boom export prosper 6 high', 'participant8 boom sale thunder 3 low', 'participant8 boom gun thunder 3 high', 'participant8 glow fire burn 2 high', 'participant8 glow face beam 5 high', 'participant8 glow skin burn 2 low', 'participant50 bow head submit 2 low', 'participant50 bow government submit 7 high', 'participant50 boom noise thunder 3 high', 'participant50 boom export thunder 4 low', 'participant50 boom gun prosper 2 low', 'participant50 boom sale prosper 6 high', 'participant50 glow fire beam 7 low', 'participant50 glow face burn 3 low', 'participant50 glow skin beam 7 high']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_phrase_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bow',\n",
       " 'butler',\n",
       " 'submit',\n",
       " 'company',\n",
       " 'boom',\n",
       " 'noise',\n",
       " 'prosper',\n",
       " 'export',\n",
       " 'sale',\n",
       " 'thunder',\n",
       " 'gun',\n",
       " 'glow',\n",
       " 'fire',\n",
       " 'burn',\n",
       " 'face',\n",
       " 'beam',\n",
       " 'skin',\n",
       " 'head',\n",
       " 'government']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "our_space = space_20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_phrase_space(phrase, x_names):\n",
    "\n",
    "    # first we get representations for verb and noun\n",
    "    subject_space = our_space[phrase[0]]\n",
    "    verb_space = our_space[phrase[1]]\n",
    "    \n",
    "    representation = np.zeros(len(x_names))\n",
    "    \n",
    "    \n",
    "    #isnt this the same as doing something pairwise with each vector?\n",
    "    for i in range(len(x_names)):\n",
    "        \n",
    "        #print(count, word)\n",
    "      #  print(count, word)\n",
    "        # I get v^Ith element from each of the vectors\n",
    "        subject_value = subject_space[i]\n",
    "        verb_value = verb_space[i]\n",
    "        \n",
    "\n",
    "        #out = subject_value + verb_value\n",
    "        #out = subject_value * verb_value\n",
    "        \n",
    "        # 6 and 0, if we do summation, we are getting 6\n",
    "        # if we do mulitplication, we are getting 0\n",
    "        \n",
    "        #out = subject_value * 0.2 + verb_value * 0.8\n",
    "        out = subject_value * 0.0 + verb_value * 0.95 + (0.05 * subject_value * verb_value)\n",
    "\n",
    "        representation[i] = out\n",
    "        \n",
    "    return representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#should input a noun and a verb, the function should add their dimensions and output a single vector\n",
    "def veclen(vector):\n",
    "    return math.sqrt(np.sum(np.square(vector)))\n",
    "def cosine(vector1, vector2):\n",
    "    veclen1 = veclen(vector1)\n",
    "    veclen2 = veclen(vector2)\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty, the cosine is 0\n",
    "        return 0.0\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        dotproduct = np.dot(vector1, vector2)\n",
    "        return dotproduct / (veclen1 * veclen2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get high and low values with words present in semantic space\n",
    "lows = []\n",
    "highs = []\n",
    "for item in cleaned_phrase_dataset[1:]:\n",
    "    item = item.split()\n",
    "    participant = item[0]\n",
    "    verb = item[1]\n",
    "    noun = item[2]\n",
    "    landmark = item[3]\n",
    "    inp = item[4]\n",
    "    hilo = item[5]\n",
    "    if hilo == \"high\":\n",
    "        highs.append(item)\n",
    "    elif hilo == \"low\":\n",
    "        lows.append(item)\n",
    "    \n",
    "\n",
    "def veclen(vector):\n",
    "    return math.sqrt(np.sum(np.square(vector)))\n",
    "def average_cosine(list_x):\n",
    "    values = 0\n",
    "    for item in list_x:\n",
    "        verb = item[1]\n",
    "        noun = item[2]\n",
    "        landmark = item[3]\n",
    "        \n",
    "        reference = [noun, verb]\n",
    "        landmark = [noun, landmark]\n",
    "        \n",
    "        ref = build_phrase_space(reference, ktw)\n",
    "        X = build_phrase_space(landmark, ktw)\n",
    "        cosine_value = cosine(ref, X)\n",
    "        values += cosine_value\n",
    "        \n",
    "    return values / len(list_x)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7250612423113745"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reference = ['face', 'glow']\n",
    "landmark_high = ['face', 'beam']\n",
    "landmark_low = ['face', 'burn']\n",
    "\n",
    "ref = build_phrase_space(reference, ktw)\n",
    "\n",
    "lhigh = build_phrase_space(landmark_high, ktw)\n",
    "\n",
    "llow = build_phrase_space(landmark_low, ktw)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cosine(ref, llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5889442468718049"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cosine(ref, lhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5431125524977768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_cosine(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4933942062382572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_cosine(lows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "  - [1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721732, Baltimore, Maryland, USA, June 2325 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "  - [2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236244). Association for Computational Linguistics.\n",
    "  \n",
    "  - [3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 31113119, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 60 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
